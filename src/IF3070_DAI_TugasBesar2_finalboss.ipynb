{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3070 Foundations of Artificial Intelligence | Tugas Besar 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: 35\n",
    "\n",
    "Group Members:\n",
    "- Angelica Aliwinata (18222113)\n",
    "- Jason Jahja (18222116)\n",
    "- Melissa Trenggono (18222123)\n",
    "- Anindita Widya Santoso (18222128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import tools for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import tools for imputing data\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Import tools for data counting\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-IWFJ-gdLfHD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>URL</th>\n",
       "      <th>URLLength</th>\n",
       "      <th>Domain</th>\n",
       "      <th>DomainLength</th>\n",
       "      <th>IsDomainIP</th>\n",
       "      <th>TLD</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>TLDLegitimateProb</th>\n",
       "      <th>...</th>\n",
       "      <th>Pay</th>\n",
       "      <th>Crypto</th>\n",
       "      <th>HasCopyrightInfo</th>\n",
       "      <th>NoOfImage</th>\n",
       "      <th>NoOfCSS</th>\n",
       "      <th>NoOfJS</th>\n",
       "      <th>NoOfSelfRef</th>\n",
       "      <th>NoOfEmptyRef</th>\n",
       "      <th>NoOfExternalRef</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.northcm.ac.th</td>\n",
       "      <td>24.0</td>\n",
       "      <td>www.northcm.ac.th</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>8135291.txt</td>\n",
       "      <td>http://uqr.to/1il1z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>586561.txt</td>\n",
       "      <td>https://www.woolworthsrewards.com.au</td>\n",
       "      <td>35.0</td>\n",
       "      <td>www.woolworthsrewards.com.au</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>au</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>412632.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.nyprowrestling.com</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     FILENAME                                   URL  URLLength  \\\n",
       "0   1          NaN             https://www.northcm.ac.th       24.0   \n",
       "1   4  8135291.txt                   http://uqr.to/1il1z        NaN   \n",
       "2   5   586561.txt  https://www.woolworthsrewards.com.au       35.0   \n",
       "3   6          NaN                                   NaN       31.0   \n",
       "4  11   412632.txt                                   NaN        NaN   \n",
       "\n",
       "                         Domain  DomainLength  IsDomainIP  TLD  \\\n",
       "0             www.northcm.ac.th          17.0         0.0  NaN   \n",
       "1                           NaN           NaN         NaN   to   \n",
       "2  www.woolworthsrewards.com.au          28.0         0.0   au   \n",
       "3                           NaN           NaN         NaN  com   \n",
       "4        www.nyprowrestling.com          22.0         0.0  NaN   \n",
       "\n",
       "   CharContinuationRate  TLDLegitimateProb  ...  Pay  Crypto  \\\n",
       "0              0.800000                NaN  ...  0.0     0.0   \n",
       "1              1.000000           0.000896  ...  NaN     0.0   \n",
       "2              0.857143                NaN  ...  1.0     0.0   \n",
       "3              0.562500           0.522907  ...  1.0     0.0   \n",
       "4              1.000000                NaN  ...  0.0     0.0   \n",
       "\n",
       "   HasCopyrightInfo  NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  \\\n",
       "0               1.0        NaN      3.0     NaN         69.0           NaN   \n",
       "1               0.0        NaN      NaN     NaN          NaN           NaN   \n",
       "2               1.0       33.0      7.0     8.0         15.0           NaN   \n",
       "3               1.0       24.0      5.0    14.0          NaN           NaN   \n",
       "4               1.0        NaN      NaN    14.0          NaN           0.0   \n",
       "\n",
       "   NoOfExternalRef  label  \n",
       "0              NaN      1  \n",
       "1              1.0      0  \n",
       "2              2.0      1  \n",
       "3              NaN      1  \n",
       "4              NaN      1  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV files from a gdrive link {Google Collab}\n",
    "train_df = pd.read_csv('https://drive.google.com/uc?id=1e1DOnQc-8_12ytIxqM8HjQNS9-q99IWJ')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140404 entries, 0 to 140403\n",
      "Data columns (total 56 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   id                          140404 non-null  int64  \n",
      " 1   FILENAME                    82872 non-null   object \n",
      " 2   URL                         96917 non-null   object \n",
      " 3   URLLength                   79765 non-null   float64\n",
      " 4   Domain                      70207 non-null   object \n",
      " 5   DomainLength                94085 non-null   float64\n",
      " 6   IsDomainIP                  98274 non-null   float64\n",
      " 7   TLD                         95005 non-null   object \n",
      " 8   CharContinuationRate        92362 non-null   float64\n",
      " 9   TLDLegitimateProb           87531 non-null   float64\n",
      " 10  URLCharProb                 88333 non-null   float64\n",
      " 11  TLDLength                   92673 non-null   float64\n",
      " 12  NoOfSubDomain               96344 non-null   float64\n",
      " 13  HasObfuscation              74684 non-null   float64\n",
      " 14  NoOfObfuscatedChar          73606 non-null   float64\n",
      " 15  ObfuscationRatio            75806 non-null   float64\n",
      " 16  NoOfLettersInURL            77066 non-null   float64\n",
      " 17  LetterRatioInURL            74658 non-null   float64\n",
      " 18  NoOfDegitsInURL             81594 non-null   float64\n",
      " 19  DegitRatioInURL             86896 non-null   float64\n",
      " 20  NoOfEqualsInURL             78826 non-null   float64\n",
      " 21  NoOfQMarkInURL              96303 non-null   float64\n",
      " 22  NoOfAmpersandInURL          95017 non-null   float64\n",
      " 23  NoOfOtherSpecialCharsInURL  92775 non-null   float64\n",
      " 24  SpacialCharRatioInURL       77570 non-null   float64\n",
      " 25  IsHTTPS                     91042 non-null   float64\n",
      " 26  LineOfCode                  71251 non-null   float64\n",
      " 27  LargestLineLength           72476 non-null   float64\n",
      " 28  HasTitle                    95825 non-null   float64\n",
      " 29  Title                       82157 non-null   object \n",
      " 30  DomainTitleMatchScore       90407 non-null   float64\n",
      " 31  URLTitleMatchScore          88188 non-null   float64\n",
      " 32  HasFavicon                  81982 non-null   float64\n",
      " 33  Robots                      93672 non-null   float64\n",
      " 34  IsResponsive                97862 non-null   float64\n",
      " 35  NoOfURLRedirect             73020 non-null   float64\n",
      " 36  NoOfSelfRedirect            73689 non-null   float64\n",
      " 37  HasDescription              85765 non-null   float64\n",
      " 38  NoOfPopup                   97051 non-null   float64\n",
      " 39  NoOfiFrame                  90460 non-null   float64\n",
      " 40  HasExternalFormSubmit       84812 non-null   float64\n",
      " 41  HasSocialNet                72405 non-null   float64\n",
      " 42  HasSubmitButton             78784 non-null   float64\n",
      " 43  HasHiddenFields             96609 non-null   float64\n",
      " 44  HasPasswordField            73869 non-null   float64\n",
      " 45  Bank                        85408 non-null   float64\n",
      " 46  Pay                         97230 non-null   float64\n",
      " 47  Crypto                      90207 non-null   float64\n",
      " 48  HasCopyrightInfo            73059 non-null   float64\n",
      " 49  NoOfImage                   89932 non-null   float64\n",
      " 50  NoOfCSS                     73270 non-null   float64\n",
      " 51  NoOfJS                      79603 non-null   float64\n",
      " 52  NoOfSelfRef                 92272 non-null   float64\n",
      " 53  NoOfEmptyRef                97718 non-null   float64\n",
      " 54  NoOfExternalRef             71025 non-null   float64\n",
      " 55  label                       140404 non-null  int64  \n",
      "dtypes: float64(49), int64(2), object(5)\n",
      "memory usage: 60.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>URL</th>\n",
       "      <th>URLLength</th>\n",
       "      <th>Domain</th>\n",
       "      <th>DomainLength</th>\n",
       "      <th>IsDomainIP</th>\n",
       "      <th>TLD</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>TLDLegitimateProb</th>\n",
       "      <th>...</th>\n",
       "      <th>Pay</th>\n",
       "      <th>Crypto</th>\n",
       "      <th>HasCopyrightInfo</th>\n",
       "      <th>NoOfImage</th>\n",
       "      <th>NoOfCSS</th>\n",
       "      <th>NoOfJS</th>\n",
       "      <th>NoOfSelfRef</th>\n",
       "      <th>NoOfEmptyRef</th>\n",
       "      <th>NoOfExternalRef</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140404.000000</td>\n",
       "      <td>82872</td>\n",
       "      <td>96917</td>\n",
       "      <td>79765.000000</td>\n",
       "      <td>70207</td>\n",
       "      <td>94085.000000</td>\n",
       "      <td>98274.000000</td>\n",
       "      <td>95005</td>\n",
       "      <td>92362.000000</td>\n",
       "      <td>87531.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>97230.000000</td>\n",
       "      <td>90207.000000</td>\n",
       "      <td>73059.000000</td>\n",
       "      <td>89932.000000</td>\n",
       "      <td>73270.000000</td>\n",
       "      <td>79603.000000</td>\n",
       "      <td>92272.000000</td>\n",
       "      <td>97718.000000</td>\n",
       "      <td>71025.000000</td>\n",
       "      <td>140404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>82872</td>\n",
       "      <td>96914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>464776.txt</td>\n",
       "      <td>http://www.strangled.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ipfs.io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>117682.632746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.701473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.619387</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.277482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345706</td>\n",
       "      <td>0.034454</td>\n",
       "      <td>0.751543</td>\n",
       "      <td>41.647489</td>\n",
       "      <td>10.268623</td>\n",
       "      <td>16.461729</td>\n",
       "      <td>104.875900</td>\n",
       "      <td>3.733846</td>\n",
       "      <td>78.104273</td>\n",
       "      <td>0.924831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>68122.005080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.140676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.833808</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.159142</td>\n",
       "      <td>0.248115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.182393</td>\n",
       "      <td>0.432121</td>\n",
       "      <td>102.538702</td>\n",
       "      <td>133.321659</td>\n",
       "      <td>21.001274</td>\n",
       "      <td>219.137035</td>\n",
       "      <td>24.192458</td>\n",
       "      <td>187.106965</td>\n",
       "      <td>0.263664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58689.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>117421.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>176724.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>235795.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4054.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8956.000000</td>\n",
       "      <td>35820.000000</td>\n",
       "      <td>2828.000000</td>\n",
       "      <td>26596.000000</td>\n",
       "      <td>4887.000000</td>\n",
       "      <td>27516.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id    FILENAME                       URL     URLLength  \\\n",
       "count   140404.000000       82872                     96917  79765.000000   \n",
       "unique            NaN       82872                     96914           NaN   \n",
       "top               NaN  464776.txt  http://www.strangled.net           NaN   \n",
       "freq              NaN           1                         2           NaN   \n",
       "mean    117682.632746         NaN                       NaN     27.701473   \n",
       "std      68122.005080         NaN                       NaN     23.140676   \n",
       "min          1.000000         NaN                       NaN     14.000000   \n",
       "25%      58689.500000         NaN                       NaN     23.000000   \n",
       "50%     117421.500000         NaN                       NaN     26.000000   \n",
       "75%     176724.250000         NaN                       NaN     30.000000   \n",
       "max     235795.000000         NaN                       NaN   4054.000000   \n",
       "\n",
       "         Domain  DomainLength    IsDomainIP    TLD  CharContinuationRate  \\\n",
       "count     70207  94085.000000  98274.000000  95005          92362.000000   \n",
       "unique    69832           NaN           NaN    497                   NaN   \n",
       "top     ipfs.io           NaN           NaN    com                   NaN   \n",
       "freq         66           NaN           NaN  47878                   NaN   \n",
       "mean        NaN     19.619387      0.000488    NaN              0.918007   \n",
       "std         NaN      5.833808      0.022095    NaN              0.159142   \n",
       "min         NaN      4.000000      0.000000    NaN              0.000000   \n",
       "25%         NaN     16.000000      0.000000    NaN              0.913043   \n",
       "50%         NaN     19.000000      0.000000    NaN              1.000000   \n",
       "75%         NaN     23.000000      0.000000    NaN              1.000000   \n",
       "max         NaN     93.000000      1.000000    NaN              1.000000   \n",
       "\n",
       "        TLDLegitimateProb  ...           Pay        Crypto  HasCopyrightInfo  \\\n",
       "count        87531.000000  ...  97230.000000  90207.000000      73059.000000   \n",
       "unique                NaN  ...           NaN           NaN               NaN   \n",
       "top                   NaN  ...           NaN           NaN               NaN   \n",
       "freq                  NaN  ...           NaN           NaN               NaN   \n",
       "mean             0.277482  ...      0.345706      0.034454          0.751543   \n",
       "std              0.248115  ...      0.475600      0.182393          0.432121   \n",
       "min              0.000000  ...      0.000000      0.000000          0.000000   \n",
       "25%              0.012927  ...      0.000000      0.000000          1.000000   \n",
       "50%              0.522907  ...      0.000000      0.000000          1.000000   \n",
       "75%              0.522907  ...      1.000000      0.000000          1.000000   \n",
       "max              0.522907  ...      1.000000      1.000000          1.000000   \n",
       "\n",
       "           NoOfImage       NoOfCSS        NoOfJS   NoOfSelfRef  NoOfEmptyRef  \\\n",
       "count   89932.000000  73270.000000  79603.000000  92272.000000  97718.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean       41.647489     10.268623     16.461729    104.875900      3.733846   \n",
       "std       102.538702    133.321659     21.001274    219.137035     24.192458   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        10.000000      2.000000      6.000000     24.000000      0.000000   \n",
       "50%        23.000000      5.000000     12.000000     69.000000      0.000000   \n",
       "75%        45.000000     12.000000     22.000000    132.000000      3.000000   \n",
       "max      8956.000000  35820.000000   2828.000000  26596.000000   4887.000000   \n",
       "\n",
       "        NoOfExternalRef          label  \n",
       "count      71025.000000  140404.000000  \n",
       "unique              NaN            NaN  \n",
       "top                 NaN            NaN  \n",
       "freq                NaN            NaN  \n",
       "mean          78.104273       0.924831  \n",
       "std          187.106965       0.263664  \n",
       "min            0.000000       0.000000  \n",
       "25%           13.000000       1.000000  \n",
       "50%           39.000000       1.000000  \n",
       "75%           99.000000       1.000000  \n",
       "max        27516.000000       1.000000  \n",
       "\n",
       "[11 rows x 56 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>URLLength</th>\n",
       "      <th>DomainLength</th>\n",
       "      <th>IsDomainIP</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>TLDLegitimateProb</th>\n",
       "      <th>URLCharProb</th>\n",
       "      <th>TLDLength</th>\n",
       "      <th>NoOfSubDomain</th>\n",
       "      <th>HasObfuscation</th>\n",
       "      <th>...</th>\n",
       "      <th>Pay</th>\n",
       "      <th>Crypto</th>\n",
       "      <th>HasCopyrightInfo</th>\n",
       "      <th>NoOfImage</th>\n",
       "      <th>NoOfCSS</th>\n",
       "      <th>NoOfJS</th>\n",
       "      <th>NoOfSelfRef</th>\n",
       "      <th>NoOfEmptyRef</th>\n",
       "      <th>NoOfExternalRef</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140404.000000</td>\n",
       "      <td>79765.000000</td>\n",
       "      <td>94085.000000</td>\n",
       "      <td>98274.000000</td>\n",
       "      <td>92362.000000</td>\n",
       "      <td>87531.000000</td>\n",
       "      <td>88333.000000</td>\n",
       "      <td>92673.000000</td>\n",
       "      <td>96344.000000</td>\n",
       "      <td>74684.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>97230.000000</td>\n",
       "      <td>90207.000000</td>\n",
       "      <td>73059.000000</td>\n",
       "      <td>89932.000000</td>\n",
       "      <td>73270.000000</td>\n",
       "      <td>79603.000000</td>\n",
       "      <td>92272.000000</td>\n",
       "      <td>97718.000000</td>\n",
       "      <td>71025.000000</td>\n",
       "      <td>140404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>117682.632746</td>\n",
       "      <td>27.701473</td>\n",
       "      <td>19.619387</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.277482</td>\n",
       "      <td>0.059286</td>\n",
       "      <td>2.729015</td>\n",
       "      <td>1.161442</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345706</td>\n",
       "      <td>0.034454</td>\n",
       "      <td>0.751543</td>\n",
       "      <td>41.647489</td>\n",
       "      <td>10.268623</td>\n",
       "      <td>16.461729</td>\n",
       "      <td>104.875900</td>\n",
       "      <td>3.733846</td>\n",
       "      <td>78.104273</td>\n",
       "      <td>0.924831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>68122.005080</td>\n",
       "      <td>23.140676</td>\n",
       "      <td>5.833808</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.159142</td>\n",
       "      <td>0.248115</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.533310</td>\n",
       "      <td>0.445054</td>\n",
       "      <td>0.020038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.182393</td>\n",
       "      <td>0.432121</td>\n",
       "      <td>102.538702</td>\n",
       "      <td>133.321659</td>\n",
       "      <td>21.001274</td>\n",
       "      <td>219.137035</td>\n",
       "      <td>24.192458</td>\n",
       "      <td>187.106965</td>\n",
       "      <td>0.263664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58689.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.055542</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>117421.500000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>0.060264</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>176724.250000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>235795.000000</td>\n",
       "      <td>4054.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>0.088766</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8956.000000</td>\n",
       "      <td>35820.000000</td>\n",
       "      <td>2828.000000</td>\n",
       "      <td>26596.000000</td>\n",
       "      <td>4887.000000</td>\n",
       "      <td>27516.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     URLLength  DomainLength    IsDomainIP  \\\n",
       "count  140404.000000  79765.000000  94085.000000  98274.000000   \n",
       "mean   117682.632746     27.701473     19.619387      0.000488   \n",
       "std     68122.005080     23.140676      5.833808      0.022095   \n",
       "min         1.000000     14.000000      4.000000      0.000000   \n",
       "25%     58689.500000     23.000000     16.000000      0.000000   \n",
       "50%    117421.500000     26.000000     19.000000      0.000000   \n",
       "75%    176724.250000     30.000000     23.000000      0.000000   \n",
       "max    235795.000000   4054.000000     93.000000      1.000000   \n",
       "\n",
       "       CharContinuationRate  TLDLegitimateProb   URLCharProb     TLDLength  \\\n",
       "count          92362.000000       87531.000000  88333.000000  92673.000000   \n",
       "mean               0.918007           0.277482      0.059286      2.729015   \n",
       "std                0.159142           0.248115      0.008063      0.533310   \n",
       "min                0.000000           0.000000      0.001229      2.000000   \n",
       "25%                0.913043           0.012927      0.055542      2.000000   \n",
       "50%                1.000000           0.522907      0.060264      3.000000   \n",
       "75%                1.000000           0.522907      0.064391      3.000000   \n",
       "max                1.000000           0.522907      0.088766     13.000000   \n",
       "\n",
       "       NoOfSubDomain  HasObfuscation  ...           Pay        Crypto  \\\n",
       "count   96344.000000    74684.000000  ...  97230.000000  90207.000000   \n",
       "mean        1.161442        0.000402  ...      0.345706      0.034454   \n",
       "std         0.445054        0.020038  ...      0.475600      0.182393   \n",
       "min         0.000000        0.000000  ...      0.000000      0.000000   \n",
       "25%         1.000000        0.000000  ...      0.000000      0.000000   \n",
       "50%         1.000000        0.000000  ...      0.000000      0.000000   \n",
       "75%         1.000000        0.000000  ...      1.000000      0.000000   \n",
       "max         7.000000        1.000000  ...      1.000000      1.000000   \n",
       "\n",
       "       HasCopyrightInfo     NoOfImage       NoOfCSS        NoOfJS  \\\n",
       "count      73059.000000  89932.000000  73270.000000  79603.000000   \n",
       "mean           0.751543     41.647489     10.268623     16.461729   \n",
       "std            0.432121    102.538702    133.321659     21.001274   \n",
       "min            0.000000      0.000000      0.000000      0.000000   \n",
       "25%            1.000000     10.000000      2.000000      6.000000   \n",
       "50%            1.000000     23.000000      5.000000     12.000000   \n",
       "75%            1.000000     45.000000     12.000000     22.000000   \n",
       "max            1.000000   8956.000000  35820.000000   2828.000000   \n",
       "\n",
       "        NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef          label  \n",
       "count  92272.000000  97718.000000     71025.000000  140404.000000  \n",
       "mean     104.875900      3.733846        78.104273       0.924831  \n",
       "std      219.137035     24.192458       187.106965       0.263664  \n",
       "min        0.000000      0.000000         0.000000       0.000000  \n",
       "25%       24.000000      0.000000        13.000000       1.000000  \n",
       "50%       69.000000      0.000000        39.000000       1.000000  \n",
       "75%      132.000000      3.000000        99.000000       1.000000  \n",
       "max    26596.000000   4887.000000     27516.000000       1.000000  \n",
       "\n",
       "[8 rows x 51 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx-gT3bLfHM"
   },
   "source": [
    "# 1. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train` folder given by the TA. The `test` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yWCUFFBLfHM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (112323, 56)\n",
      "Validation Set: (28081, 56)\n"
     ]
    }
   ],
   "source": [
    "# Keep original training set\n",
    "train_set_copy = train_df.copy()\n",
    "\n",
    "# Splitting training and validation set\n",
    "train_df, validate_set = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check training and validation set size\n",
    "print(\"Training Set:\", train_df.shape)\n",
    "print(\"Validation Set:\", validate_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC14lmo_LfHN"
   },
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "We will give some common methods for you to try, but you only have to **at least implement one method for each process**. For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95_A8hSLfHY"
   },
   "source": [
    "## A. Data Cleaning\n",
    "\n",
    "**Data cleaning** is the crucial first step in preparing your dataset for machine learning. Raw data collected from various sources is often messy and may contain errors, missing values, and inconsistencies. Data cleaning involves the following steps:\n",
    "\n",
    "1. **Handling Missing Data:** Identify and address missing values in the dataset. This can include imputing missing values, removing rows or columns with excessive missing data, or using more advanced techniques like interpolation.\n",
    "\n",
    "2. **Dealing with Outliers:** Identify and handle outliers, which are data points significantly different from the rest of the dataset. Outliers can be removed or transformed to improve model performance.\n",
    "\n",
    "3. **Data Validation:** Check for data integrity and consistency. Ensure that data types are correct, categorical variables have consistent labels, and numerical values fall within expected ranges.\n",
    "\n",
    "4. **Removing Duplicates:** Identify and remove duplicate rows, as they can skew the model's training process and evaluation metrics.\n",
    "\n",
    "5. **Feature Engineering**: Create new features or modify existing ones to extract relevant information. This step can involve scaling, normalizing, or encoding features for better model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wruGao9LfHZ"
   },
   "source": [
    "### I. Handling Missing Data\n",
    "\n",
    "Missing data can adversely affect the performance and accuracy of machine learning models. There are several strategies to handle missing data in machine learning:\n",
    "\n",
    "1. **Data Imputation:**\n",
    "\n",
    "    a. **Mean, Median, or Mode Imputation:** For numerical features, you can replace missing values with the mean, median, or mode of the non-missing values in the same feature. This method is simple and often effective when data is missing at random.\n",
    "\n",
    "    b. **Constant Value Imputation:** You can replace missing values with a predefined constant value (e.g., 0) if it makes sense for your dataset and problem.\n",
    "\n",
    "    c. **Imputation Using Predictive Models:** More advanced techniques involve using predictive models to estimate missing values. For example, you can train a regression model to predict missing numerical values or a classification model to predict missing categorical values.\n",
    "\n",
    "2. **Deletion of Missing Data:**\n",
    "\n",
    "    a. **Listwise Deletion:** In cases where the amount of missing data is relatively small, you can simply remove rows with missing values from your dataset. However, this approach can lead to a loss of valuable information.\n",
    "\n",
    "    b. **Column (Feature) Deletion:** If a feature has a large number of missing values and is not critical for your analysis, you can consider removing that feature altogether.\n",
    "\n",
    "3. **Domain-Specific Strategies:**\n",
    "\n",
    "    a. **Domain Knowledge:** In some cases, domain knowledge can guide the imputation process. For example, if you know that missing values are related to a specific condition, you can impute them accordingly.\n",
    "\n",
    "4. **Imputation Libraries:**\n",
    "\n",
    "    a. **Scikit-Learn:** Scikit-Learn provides a `SimpleImputer` class that can handle basic imputation strategies like mean, median, and mode imputation.\n",
    "\n",
    "    b. **Fancyimpute:** Fancyimpute is a Python library that offers more advanced imputation techniques, including matrix factorization, k-nearest neighbors, and deep learning-based methods.\n",
    "\n",
    "The choice of imputation method should be guided by the nature of your data, the amount of missing data, the problem you are trying to solve, and the assumptions you are willing to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ucZNfCkiLfHZ"
   },
   "outputs": [],
   "source": [
    "# Split numerical and non-numerical columns on training set\n",
    "numerical_columns = train_df.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_columns = train_df.select_dtypes(include=['object'])\n",
    "\n",
    "# Split numerical and non-numerical columns on validation set\n",
    "numerical_columns = validate_set.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_columns = validate_set.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "def missing_vals(column, dataset):\n",
    "    \n",
    "    # Count total missing values in the column\n",
    "    total_missing = column.isnull().sum()\n",
    "    \n",
    "    # Filter columns with missing values and sort by descending order\n",
    "    columns_with_missing = total_missing[total_missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    # Loop through each column with missing values and calculate percentage\n",
    "    for column_name in columns_with_missing.index:\n",
    "        missing_count = len(dataset) - dataset[column_name].value_counts().sum()\n",
    "        missing_percentage = round((missing_count / len(dataset)) * 100, 2)\n",
    "        print(f\"{column_name} has {missing_percentage}% missing values ({missing_count} rows missing).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing numerical data on training set: \n",
      "LineOfCode has 49.18% missing values (55243 rows missing).\n",
      "NoOfExternalRef has 49.41% missing values (55498 rows missing).\n",
      "LargestLineLength has 48.38% missing values (54339 rows missing).\n",
      "NoOfCSS has 47.67% missing values (53549 rows missing).\n",
      "HasSocialNet has 48.5% missing values (54473 rows missing).\n",
      "HasCopyrightInfo has 47.99% missing values (53909 rows missing).\n",
      "NoOfURLRedirect has 48.06% missing values (53986 rows missing).\n",
      "NoOfObfuscatedChar has 47.59% missing values (53460 rows missing).\n",
      "HasObfuscation has 46.76% missing values (52518 rows missing).\n",
      "HasPasswordField has 47.49% missing values (53344 rows missing).\n",
      "NoOfSelfRedirect has 47.67% missing values (53549 rows missing).\n",
      "LetterRatioInURL has 46.87% missing values (52644 rows missing).\n",
      "ObfuscationRatio has 46.04% missing values (51714 rows missing).\n",
      "NoOfLettersInURL has 45.09% missing values (50650 rows missing).\n",
      "SpacialCharRatioInURL has 44.69% missing values (50197 rows missing).\n",
      "HasSubmitButton has 43.77% missing values (49168 rows missing).\n",
      "NoOfEqualsInURL has 43.83% missing values (49236 rows missing).\n",
      "NoOfJS has 43.32% missing values (48660 rows missing).\n",
      "URLLength has 43.19% missing values (48513 rows missing).\n",
      "NoOfDegitsInURL has 41.77% missing values (46915 rows missing).\n",
      "HasFavicon has 41.65% missing values (46786 rows missing).\n",
      "HasExternalFormSubmit has 39.69% missing values (44579 rows missing).\n",
      "HasDescription has 38.89% missing values (43685 rows missing).\n",
      "Bank has 39.24% missing values (44070 rows missing).\n",
      "DegitRatioInURL has 38.17% missing values (42874 rows missing).\n",
      "URLTitleMatchScore has 37.17% missing values (41745 rows missing).\n",
      "TLDLegitimateProb has 37.77% missing values (42428 rows missing).\n",
      "URLCharProb has 37.19% missing values (41769 rows missing).\n",
      "NoOfiFrame has 35.43% missing values (39801 rows missing).\n",
      "NoOfImage has 35.92% missing values (40342 rows missing).\n",
      "DomainTitleMatchScore has 35.57% missing values (39957 rows missing).\n",
      "Crypto has 35.81% missing values (40220 rows missing).\n",
      "IsHTTPS has 35.11% missing values (39438 rows missing).\n",
      "NoOfSelfRef has 34.24% missing values (38461 rows missing).\n",
      "CharContinuationRate has 34.17% missing values (38384 rows missing).\n",
      "NoOfOtherSpecialCharsInURL has 33.85% missing values (38026 rows missing).\n",
      "TLDLength has 34.0% missing values (38186 rows missing).\n",
      "DomainLength has 32.92% missing values (36975 rows missing).\n",
      "Robots has 33.4% missing values (37518 rows missing).\n",
      "NoOfAmpersandInURL has 32.31% missing values (36289 rows missing).\n",
      "HasTitle has 31.69% missing values (35597 rows missing).\n",
      "NoOfQMarkInURL has 31.36% missing values (35222 rows missing).\n",
      "HasHiddenFields has 31.15% missing values (34988 rows missing).\n",
      "NoOfSubDomain has 31.51% missing values (35398 rows missing).\n",
      "NoOfPopup has 30.96% missing values (34780 rows missing).\n",
      "NoOfEmptyRef has 30.37% missing values (34114 rows missing).\n",
      "Pay has 30.85% missing values (34647 rows missing).\n",
      "IsResponsive has 30.31% missing values (34048 rows missing).\n",
      "IsDomainIP has 29.98% missing values (33678 rows missing).\n"
     ]
    }
   ],
   "source": [
    "# Missing values on numerical columns\n",
    "print(\"Missing numerical data on training set: \")\n",
    "missing_vals(numerical_columns,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing categorical data on training set: \n",
      "Domain has 50.0% missing values (56165 rows missing).\n",
      "FILENAME has 40.9% missing values (45942 rows missing).\n",
      "Title has 41.57% missing values (46690 rows missing).\n",
      "TLD has 32.31% missing values (36286 rows missing).\n",
      "URL has 30.87% missing values (34672 rows missing).\n"
     ]
    }
   ],
   "source": [
    "# Missing values on categorical columns\n",
    "print(\"Missing categorical data on training set: \")\n",
    "missing_vals(categorical_columns, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing numerical data on validation set: \n",
      "LineOfCode has 49.54% missing values (13910 rows missing).\n",
      "NoOfExternalRef has 49.43% missing values (13881 rows missing).\n",
      "LargestLineLength has 48.39% missing values (13589 rows missing).\n",
      "NoOfCSS has 48.38% missing values (13585 rows missing).\n",
      "HasSocialNet has 48.17% missing values (13526 rows missing).\n",
      "HasCopyrightInfo has 47.85% missing values (13436 rows missing).\n",
      "NoOfURLRedirect has 47.71% missing values (13398 rows missing).\n",
      "NoOfObfuscatedChar has 47.5% missing values (13338 rows missing).\n",
      "HasObfuscation has 47.01% missing values (13202 rows missing).\n",
      "HasPasswordField has 46.97% missing values (13191 rows missing).\n",
      "NoOfSelfRedirect has 46.89% missing values (13166 rows missing).\n",
      "LetterRatioInURL has 46.66% missing values (13102 rows missing).\n",
      "ObfuscationRatio has 45.88% missing values (12884 rows missing).\n",
      "NoOfLettersInURL has 45.18% missing values (12688 rows missing).\n",
      "SpacialCharRatioInURL has 45.0% missing values (12637 rows missing).\n",
      "HasSubmitButton has 44.34% missing values (12452 rows missing).\n",
      "NoOfEqualsInURL has 43.95% missing values (12342 rows missing).\n",
      "NoOfJS has 43.24% missing values (12141 rows missing).\n",
      "URLLength has 43.18% missing values (12126 rows missing).\n",
      "NoOfDegitsInURL has 42.36% missing values (11895 rows missing).\n",
      "HasFavicon has 41.44% missing values (11636 rows missing).\n",
      "HasExternalFormSubmit has 39.22% missing values (11013 rows missing).\n",
      "HasDescription has 39.01% missing values (10954 rows missing).\n",
      "Bank has 38.91% missing values (10926 rows missing).\n",
      "DegitRatioInURL has 37.87% missing values (10634 rows missing).\n",
      "URLTitleMatchScore has 37.29% missing values (10471 rows missing).\n",
      "TLDLegitimateProb has 37.2% missing values (10445 rows missing).\n",
      "URLCharProb has 36.69% missing values (10302 rows missing).\n",
      "NoOfiFrame has 36.12% missing values (10143 rows missing).\n",
      "NoOfImage has 36.07% missing values (10130 rows missing).\n",
      "DomainTitleMatchScore has 35.75% missing values (10040 rows missing).\n",
      "Crypto has 35.53% missing values (9977 rows missing).\n",
      "IsHTTPS has 35.34% missing values (9924 rows missing).\n",
      "NoOfSelfRef has 34.44% missing values (9671 rows missing).\n",
      "CharContinuationRate has 34.39% missing values (9658 rows missing).\n",
      "NoOfOtherSpecialCharsInURL has 34.2% missing values (9603 rows missing).\n",
      "TLDLength has 33.99% missing values (9545 rows missing).\n",
      "DomainLength has 33.28% missing values (9344 rows missing).\n",
      "Robots has 32.81% missing values (9214 rows missing).\n",
      "NoOfAmpersandInURL has 32.4% missing values (9098 rows missing).\n",
      "HasTitle has 31.99% missing values (8982 rows missing).\n",
      "NoOfQMarkInURL has 31.62% missing values (8879 rows missing).\n",
      "HasHiddenFields has 31.36% missing values (8807 rows missing).\n",
      "NoOfSubDomain has 30.85% missing values (8662 rows missing).\n",
      "NoOfPopup has 30.53% missing values (8573 rows missing).\n",
      "NoOfEmptyRef has 30.53% missing values (8572 rows missing).\n",
      "Pay has 30.37% missing values (8527 rows missing).\n",
      "IsResponsive has 30.25% missing values (8494 rows missing).\n",
      "IsDomainIP has 30.1% missing values (8452 rows missing).\n"
     ]
    }
   ],
   "source": [
    "# Missing values on numerical columns\n",
    "print(\"Missing numerical data on validation set: \")\n",
    "missing_vals(numerical_columns,validate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing categorical data on validation set: \n",
      "Domain has 49.97% missing values (14032 rows missing).\n",
      "FILENAME has 41.27% missing values (11590 rows missing).\n",
      "Title has 41.16% missing values (11557 rows missing).\n",
      "TLD has 32.45% missing values (9113 rows missing).\n",
      "URL has 31.39% missing values (8815 rows missing).\n"
     ]
    }
   ],
   "source": [
    "# Missing values on categorical columns\n",
    "print(\"Missing categorical data on validation set: \")\n",
    "missing_vals(categorical_columns, validate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgrSMcK75VY_"
   },
   "source": [
    "### II. Dealing with Outliers\n",
    "\n",
    "Outliers are data points that significantly differ from the majority of the data. They can be unusually high or low values that do not fit the pattern of the rest of the dataset. Outliers can significantly impact model performance, so it is important to handle them properly.\n",
    "\n",
    "Some methods to handle outliers:\n",
    "1. **Imputation**: Replace with mean, median, or a boundary value.\n",
    "2. **Clipping**: Cap values to upper and lower limits.\n",
    "3. **Transformation**: Use log, square root, or power transformations to reduce their influence.\n",
    "4. **Model-Based**: Use algorithms robust to outliers (e.g., tree-based models, Huber regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CgbZ6Lv17Uf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO0ZEZ-s6Lu-"
   },
   "source": [
    "### III. Remove Duplicates\n",
    "Handling duplicate values is crucial because they can compromise data integrity, leading to inaccurate analysis and insights. Duplicate entries can bias machine learning models, causing overfitting and reducing their ability to generalize to new data. They also inflate the dataset size unnecessarily, increasing computational costs and processing times. Additionally, duplicates can distort statistical measures and lead to inconsistencies, ultimately affecting the reliability of data-driven decisions and reporting. Ensuring data quality by removing duplicates is essential for accurate, efficient, and consistent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BHCkkZ-v7iF8"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eycPASmMLfHa"
   },
   "source": [
    "### IV. Feature Engineering\n",
    "\n",
    "**Feature engineering** involves creating new features (input variables) or transforming existing ones to improve the performance of machine learning models. Feature engineering aims to enhance the model's ability to learn patterns and make accurate predictions from the data. It's often said that \"good features make good models.\"\n",
    "\n",
    "1. **Feature Selection:** Feature engineering can involve selecting the most relevant and informative features from the dataset. Removing irrelevant or redundant features not only simplifies the model but also reduces the risk of overfitting.\n",
    "\n",
    "2. **Creating New Features:** Sometimes, the existing features may not capture the underlying patterns effectively. In such cases, engineers create new features that provide additional information. For example:\n",
    "   \n",
    "   - **Polynomial Features:** Engineers may create new features by taking the square, cube, or other higher-order terms of existing numerical features. This can help capture nonlinear relationships.\n",
    "   \n",
    "   - **Interaction Features:** Interaction features are created by combining two or more existing features. For example, if you have features \"length\" and \"width,\" you can create an \"area\" feature by multiplying them.\n",
    "\n",
    "3. **Binning or Discretization:** Continuous numerical features can be divided into bins or categories. For instance, age values can be grouped into bins like \"child,\" \"adult,\" and \"senior.\"\n",
    "\n",
    "4. **Domain-Specific Feature Engineering:** Depending on the domain and problem, engineers may create domain-specific features. For example, in fraud detection, features related to transaction history and user behavior may be engineered to identify anomalies.\n",
    "\n",
    "Feature engineering is both a creative and iterative process. It requires a deep understanding of the data, domain knowledge, and experimentation to determine which features will enhance the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UoXEV6wkLfHa"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw11_49xLfHb"
   },
   "source": [
    "## B. Data Preprocessing\n",
    "\n",
    "**Data preprocessing** is a broader step that encompasses both data cleaning and additional transformations to make the data suitable for machine learning algorithms. Its primary goals are:\n",
    "\n",
    "1. **Feature Scaling:** Ensure that numerical features have similar scales. Common techniques include Min-Max scaling (scaling to a specific range) or standardization (mean-centered, unit variance).\n",
    "\n",
    "2. **Encoding Categorical Variables:** Machine learning models typically work with numerical data, so categorical variables need to be encoded. This can be done using one-hot encoding, label encoding, or more advanced methods like target encoding.\n",
    "\n",
    "3. **Handling Imbalanced Classes:** If dealing with imbalanced classes in a binary classification task, apply techniques such as oversampling, undersampling, or using different evaluation metrics to address class imbalance.\n",
    "\n",
    "4. **Dimensionality Reduction:** Reduce the number of features using techniques like Principal Component Analysis (PCA) or feature selection to simplify the model and potentially improve its performance.\n",
    "\n",
    "5. **Normalization:** Normalize data to achieve a standard distribution. This is particularly important for algorithms that assume normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVyVnA1hLfHd"
   },
   "source": [
    "### Notes on Preprocessing processes\n",
    "\n",
    "It is advised to create functions or classes that have the same/similar type of inputs and outputs, so you can add, remove, or swap the order of the processes easily. You can implement the functions or classes by yourself\n",
    "\n",
    "or\n",
    "\n",
    "use `sklearn` library. To create a new preprocessing component in `sklearn`, implement a corresponding class that includes:\n",
    "1. Inheritance to `BaseEstimator` and `TransformerMixin`\n",
    "2. The method `fit`\n",
    "3. The method `transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WbxHt-5eKz_I"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# class FeatureEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "\n",
    "#         # Fit the encoder here\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         X_encoded = X.copy()\n",
    "\n",
    "#         # Encode the categorical variables here\n",
    "\n",
    "#         return X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCgOl4xLfHb"
   },
   "source": [
    "### I. Feature Scaling\n",
    "\n",
    "**Feature scaling** is a preprocessing technique used in machine learning to standardize the range of independent variables or features of data. The primary goal of feature scaling is to ensure that all features contribute equally to the training process and that machine learning algorithms can work effectively with the data.\n",
    "\n",
    "Here are the main reasons why feature scaling is important:\n",
    "\n",
    "1. **Algorithm Sensitivity:** Many machine learning algorithms are sensitive to the scale of input features. If the scales of features are significantly different, some algorithms may perform poorly or take much longer to converge.\n",
    "\n",
    "2. **Distance-Based Algorithms:** Algorithms that rely on distances or similarities between data points, such as k-nearest neighbors (KNN) and support vector machines (SVM), can be influenced by feature scales. Features with larger scales may dominate the distance calculations.\n",
    "\n",
    "3. **Regularization:** Regularization techniques, like L1 (Lasso) and L2 (Ridge) regularization, add penalty terms based on feature coefficients. Scaling ensures that all features are treated equally in the regularization process.\n",
    "\n",
    "Common methods for feature scaling include:\n",
    "\n",
    "1. **Min-Max Scaling (Normalization):** This method scales features to a specific range, typically [0, 1]. It's done using the following formula:\n",
    "\n",
    "   $$X' = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "   - Here, $X$ is the original feature value, $X_{min}$ is the minimum value of the feature, and $X_{max}$ is the maximum value of the feature.  \n",
    "<br />\n",
    "<br />\n",
    "2. **Standardization (Z-score Scaling):** This method scales features to have a mean (average) of 0 and a standard deviation of 1. It's done using the following formula:\n",
    "\n",
    "   $$X' = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "   - $X$ is the original feature value, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation of the feature.  \n",
    "<br />\n",
    "<br />\n",
    "3. **Robust Scaling:** Robust scaling is a method that scales features to the interquartile range (IQR) and is less affected by outliers. It's calculated as:\n",
    "\n",
    "   $$X' = \\frac{X - Q1}{Q3 - Q1}$$\n",
    "\n",
    "   - $X$ is the original feature value, $Q1$ is the first quartile (25th percentile), and $Q3$ is the third quartile (75th percentile) of the feature.  \n",
    "<br />\n",
    "<br />\n",
    "4. **Log Transformation:** In cases where data is highly skewed or has a heavy-tailed distribution, taking the logarithm of the feature values can help stabilize the variance and improve scaling.\n",
    "\n",
    "The choice of scaling method depends on the characteristics of your data and the requirements of your machine learning algorithm. **Min-max scaling and standardization are the most commonly used techniques and work well for many datasets.**\n",
    "\n",
    "Scaling should be applied separately to each training and test set to prevent data leakage from the test set into the training set. Additionally, **some algorithms may not require feature scaling, particularly tree-based models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "COef9EbCLfHb"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Lh-4JwLfHc"
   },
   "source": [
    "### II. Feature Encoding\n",
    "\n",
    "**Feature encoding**, also known as **categorical encoding**, is the process of converting categorical data (non-numeric data) into a numerical format so that it can be used as input for machine learning algorithms. Most machine learning models require numerical data for training and prediction, so feature encoding is a critical step in data preprocessing.\n",
    "\n",
    "Categorical data can take various forms, including:\n",
    "\n",
    "1. **Nominal Data:** Categories with no intrinsic order, like colors or country names.  \n",
    "\n",
    "2. **Ordinal Data:** Categories with a meaningful order but not necessarily equidistant, like education levels (e.g., \"high school,\" \"bachelor's,\" \"master's\").\n",
    "\n",
    "There are several common methods for encoding categorical data:\n",
    "\n",
    "1. **Label Encoding:**\n",
    "\n",
    "   - Label encoding assigns a unique integer to each category in a feature.\n",
    "   - It's suitable for ordinal data where there's a clear order among categories.\n",
    "   - For example, if you have an \"education\" feature with values \"high school,\" \"bachelor's,\" and \"master's,\" you can encode them as 0, 1, and 2, respectively.\n",
    "<br />\n",
    "<br />\n",
    "2. **One-Hot Encoding:**\n",
    "\n",
    "   - One-hot encoding creates a binary (0 or 1) column for each category in a nominal feature.\n",
    "   - It's suitable for nominal data where there's no inherent order among categories.\n",
    "   - Each category becomes a new feature, and the presence (1) or absence (0) of a category is indicated for each row.\n",
    "<br />\n",
    "<br />\n",
    "3. **Target Encoding (Mean Encoding):**\n",
    "\n",
    "   - Target encoding replaces each category with the mean of the target variable for that category.\n",
    "   - It's often used for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "psElSUugLfHc"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKQO9wtB8Pc0"
   },
   "source": [
    "### III. Handling Imbalanced Dataset\n",
    "\n",
    "**Handling imbalanced datasets** is important because imbalanced data can lead to several issues that negatively impact the performance and reliability of machine learning models. Here are some key reasons:\n",
    "\n",
    "1. **Biased Model Performance**:\n",
    "\n",
    " - Models trained on imbalanced data tend to be biased towards the majority class, leading to poor performance on the minority class. This can result in misleading accuracy metrics.\n",
    "\n",
    "2. **Misleading Accuracy**:\n",
    "\n",
    " - High overall accuracy can be misleading in imbalanced datasets. For example, if 95% of the data belongs to one class, a model that always predicts the majority class will have 95% accuracy but will fail to identify the minority class.\n",
    "\n",
    "3. **Poor Generalization**:\n",
    "\n",
    " - Models trained on imbalanced data may not generalize well to new, unseen data, especially if the minority class is underrepresented.\n",
    "\n",
    "\n",
    "Some methods to handle imbalanced datasets:\n",
    "1. **Resampling Methods**:\n",
    "\n",
    " - Oversampling: Increase the number of instances in the minority class by duplicating or generating synthetic samples (e.g., SMOTE).\n",
    " - Undersampling: Reduce the number of instances in the majority class to balance the dataset.\n",
    "\n",
    "2. **Evaluation Metrics**:\n",
    "\n",
    " - Use appropriate evaluation metrics such as precision, recall, F1-score, ROC-AUC, and confusion matrix instead of accuracy to better assess model performance on imbalanced data.\n",
    "\n",
    "3. **Algorithmic Approaches**:\n",
    "\n",
    " - Use algorithms that are designed to handle imbalanced data, such as decision trees, random forests, or ensemble methods.\n",
    " - Adjust class weights in algorithms to give more importance to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "u2BQd2XJ9W1i"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ctVzt5DLfHd"
   },
   "source": [
    "# 3. Compile Preprocessing Pipeline\n",
    "\n",
    "All of the preprocessing classes or functions defined earlier will be compiled in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ZlncSVjJG6"
   },
   "source": [
    "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jHraoW_7LfHd"
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Note: You can add or delete preprocessing components from this pipeline\n",
    "\n",
    "# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n",
    "#                  (\"featurecreator\", FeatureCreator()),\n",
    "#                  (\"scaler\", FeatureScaler()),\n",
    "#                  (\"encoder\", FeatureEncoder())])\n",
    "\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9s56aFFxLfHd"
   },
   "outputs": [],
   "source": [
    "# # Your code should work up until this point\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXoCqMztjhr-"
   },
   "source": [
    "or create your own here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7OoZ3oXEj2CW"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3adbZXLfHe"
   },
   "source": [
    "# 4. Modeling and Validation\n",
    "\n",
    "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the target feature `label`. Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnhMNbBILfHf"
   },
   "source": [
    "## A. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV6ICmFmlqjk"
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3, metric=\"euclidean\"):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = np.array(X_train)\n",
    "        self.y_train = np.array(y_train)\n",
    "    \n",
    "    def calculate_distance(point1, point2, p=2):\n",
    "        if self.metric == \"euclidean\":\n",
    "            return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "        elif self.metric == \"manhattan\":\n",
    "            return np.sum(np.abs(point1 - point2))\n",
    "        elif self.metric == \"minkowski\":\n",
    "            return np.sum(np.abs(point1 - point2) ** p) ** (1 / p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = [self._predict_single(x) for x in X_test]\n",
    "        return predictions\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        majority_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "        return majority_label\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        return np.mean(predictions == y_test)\n",
    "    \n",
    "    def submit(self, X_test, filename='submission.csv'):\n",
    "        predictions = self.predict(X_test)\n",
    "        submission_df = pd.DataFrame({'id': test_df['id'], 'attack_cat': predictions})\n",
    "        submission_df.to_csv(filename, index=False)\n",
    "        return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_distance(point1, point2, metric=\"euclidean\", p=2):\n",
    "    if metric == \"euclidean\":\n",
    "        return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "    elif metric == \"manhattan\":\n",
    "        return np.sum(np.abs(point1 - point2))\n",
    "    elif metric == \"minkowski\":\n",
    "        return np.sum(np.abs(point1 - point2) ** p) ** (1 / p)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, metric=\"euclidean\", p=2):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = np.array(X_train)\n",
    "        self.y_train = np.array(y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for test_point in X_test:\n",
    "            distances = [\n",
    "                calculate_distance(test_point, train_point, self.metric, self.p)\n",
    "                for train_point in self.X_train\n",
    "            ]\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            majority_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(majority_label)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test, scaler, label_encoders = preprocess_data(train_df)\n",
    "\n",
    "knn_lib = KNeighborsClassifier(n_neighbors=3, metric='euclidean', weights='uniform')\n",
    "\n",
    "knn_lib.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn_lib.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, knn_lib.predict(X_test)))\n",
    "print(\"classification_report:\")\n",
    "print(classification_report(y_test, knn_lib.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW0bMzkDLfHf"
   },
   "source": [
    "## B. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "C_XwsN_-LfHg"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoH2u6fOLfHh"
   },
   "source": [
    "## C. Improvements (Optional)\n",
    "\n",
    "- **Visualize the model evaluation result**\n",
    "\n",
    "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
    "\n",
    "- **Explore the hyperparameters of your models**\n",
    "\n",
    "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
    "\n",
    "- **Cross-validation**\n",
    "\n",
    "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pg-A54yELfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li4l53DjLfHh"
   },
   "source": [
    "## D. Submission\n",
    "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
    "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
    "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
    "3. Retrain the model on the preprocessed training set\n",
    "4. Predict the test set\n",
    "5. Make sure the submission contains the `id` and `label` column.\n",
    "\n",
    "Note: Adjust step 1 and 2 to your implementation of the preprocessing step if you don't use pipeline API from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LeqnfWc-LfHi"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-jXvKOpLfHi"
   },
   "source": [
    "# 6. Error Analysis\n",
    "\n",
    "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
    "\n",
    "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
    "- To each models I have tried, which performs the best and what could be the reason?\n",
    "- Is it better for me to impute or drop the missing data? Why?\n",
    "- Does feature scaling help improve my model performance?\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWL3nEAELfHj"
   },
   "source": [
    "`Provide your analysis here`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
